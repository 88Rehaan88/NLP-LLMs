{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtJHVQ6i7jGR",
    "outputId": "bd977beb-439a-480c-e856-18af8c55cf5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrNkxohOD3_3"
   },
   "outputs": [],
   "source": [
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EdFuNYPDvTS"
   },
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLQHI1WHJDSb"
   },
   "source": [
    "# Using BERT on MRPC Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSbcG8VSxgJ3"
   },
   "source": [
    "**Diff-pruning on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e192eb78718d4aee8ba8f5a405f920f5",
      "748db8e89e0045f7a9f22a84ac25637d",
      "06a6e85de9f54e6189e23a7445406bda",
      "bf4efd3689d2403fbb6d9372a3054b5e",
      "5fca3917016a4f3ba8abe38b2df8e7e8",
      "e3e34efa25c44464bef3fbd7e6407c30",
      "4c86357acd6b4044a069b7be8e81a67f",
      "c2047a0fafc443918f27033816e2b3ab",
      "d90842a2b37346d6984752240e33fef8",
      "55e206212cd2404a87e197f2d3e6ff37",
      "a4c0c9b0a47a4f62b3a65674cb5d0da2",
      "9f63cde54b064c848b96a9b8c54e3abc",
      "a0d0c7387243470eb3eee1fe339297a3",
      "6b2018fe51fb4d289496e6d80a995787",
      "8c981ac780ae4794af1f9476601f9510",
      "b790dfa619fd4915b0524906b997ec50",
      "8c824f536d234714a435632c63841a3d",
      "2e0837dec6494af484bc50128b83bf8d",
      "2204839e2e6744efb465711ab50b9ef7",
      "ee30e11320194111af3b1940a5a40a97",
      "230deb54e13841858fd93d81370baaee",
      "91ef0b0a33464115a96f06dc540edd71",
      "d1e81e22f5944d198502319a4b833cf5",
      "df6b6b40863742e88713f1b789d5bde4",
      "6b64715b0fdb48f890d18e7a614fbc66",
      "a7d7643689234a46bb250235fbbad8b5",
      "ad032aa7664f45acbef276c17fafb357",
      "f3b37b54bbbc4f70a6ade4dc086b1865",
      "3721d3248a394a3e8beb15a6a0a006f8",
      "7b71d89155754f3daf43836661256831",
      "7de6b01848874771998ffa66f1239f46",
      "1eb0b3fa766a4cc4ad2d973c5ee489c4",
      "3fdf6ba57777477cb4d0122e12e79225",
      "575689c8893241419c5a6c40fc2737e6",
      "119b2f51b1064e90852f1a8728ee217c",
      "8bfca2f993f44c40b3b01bfe440b3073",
      "3a2333ed10d040be8d4628227b4e2b77",
      "9c18afaf4c2149528e7f7b472a5245c4",
      "5933f47d6abb44e685184ea539485577",
      "86d8cb9f49d84345aeb5b02cd47fdce4",
      "e915272745af481c97943db572fead38",
      "18f2849130ba46b58e7b436f6c2537dd",
      "9830e9e688944ea190ef412341b2d206",
      "dd91d35cd8ab4615b18036e10bedaba5",
      "285da956dda549caabb51e01bb8ab903",
      "b1026d97e1df491abd53a4b1fd12c0ee",
      "7517bb8a7d964bd2a85aa88e5d55cf45",
      "8ebb4c181faf4d039f5fe7dec5107655",
      "fb8f7446e91f41618789766d833a32e1",
      "bfe799219efe46b49ce6187d218abd8c",
      "369a7dc4bb5f4ca28f9ca0690679af07",
      "1c5420798c2f4b7ca265dd90895883da",
      "693eda14df634e3e91e4fc493b2a21cd",
      "58a16518775c406e9373d0d9bbcb926c",
      "024f1eaf3cc54153a7bf578465eb9e75",
      "a68bece4bd36416ebb08be72d9f0c3e8",
      "5761741c7f6441fb93f3bf1494040b0b",
      "6d4cf3f0ac0049abacb38cd68b1c0519",
      "bd22cb6ca50b4ec298b5756a98964021",
      "ce872eae7cf64f6ba3dd2faacbe17b2b",
      "817e612b7667405997831d127b31d2b3",
      "ff45fd48f6ff473f87de39a5b4710c22",
      "0a8d734292c54f79bf01e1a296cb7247",
      "b6790010de994558a684f2c894ac618e",
      "e0492dfd35c341b08b8d05039c1fbeb8",
      "24528cef634344d2bff5b6b502f9d397",
      "d94c23e4ff084066bcfb528339194ad8",
      "b6b4ac682f1d48498bf4d52cb8aaf6be",
      "9bb289dcbea14a40bc75a6680b6ccdba",
      "ec601cc9e85b43a28a9d8671a30d905e",
      "f46a94c3f34549629dd296887e061e96",
      "9d3382c3001c4400a6a70ad179b29c21",
      "7b93df89b8bd4389b59a7ef2113d2647",
      "d15dc8598615403d924720621a213298",
      "96b045216ca14c8a977476b6e00fd300",
      "b8c3fb6a54dd4d4095667b9a11517b9d",
      "1ab7205d0d634e02a90e9d2f95708bee",
      "372c8598981344888fdbde759447ad26",
      "9f4bd9266e87429591b2542cc49ed36c",
      "e99867279f1744f6bc08a7ce9250c399",
      "84b7963a82804eae9ed841344a70c8fd",
      "8704e9c0404a441bb29c28aa5ddf8329",
      "9177869a22c34efbbc6c4577fa35804b",
      "b088d91fd44c407eaa151cb22a4c7d85",
      "82be386b5ec746c6a1a28dbdc25c0a5f",
      "f9d9aeef10104665a47fd35112ae6e90",
      "278eccc381ba4ac78376fe1fef0b978c",
      "460f494efabb44a798005255f068eced",
      "1c344bc4cd3047388ee02842fb503ce7",
      "ea9613609963424aa02232886a5b7799",
      "72f13c67a0f14dff88bf2acaedd1ad86",
      "3406e0ad773e4df3b9f38ce58fab175b",
      "6a24bcc35a4346f7b2b27053e175d6db",
      "8ddcfd21028f43f4a79bc48dea9f8f5c",
      "6af71f656781424b92961a6295110635",
      "0f28a046f4f543e2ac5b91173a105283",
      "a460f7897db54c3ab38652f24523bb4e",
      "8cee0a43011c445cb027de72ef8012ce",
      "11adb6a2cedc400b954f08218306fe3c",
      "f652fc455e46416081d5938d65c3885e",
      "0f96b108e71d47cf93f5b5ad9c816e89",
      "1f27ad5efec74050b12746da6b0a0528",
      "fa679eadd3ef4264ba81573dd79d1e7e",
      "a579a3e2d6844288b64b3ef519f43c22",
      "90fc66f9ccbb473699c1392e9f0a85e6",
      "45a022ba1da6420bb9b5a861886cc876",
      "a71440c11aa245a98d74fc057043bd1a",
      "1ee4fec5081b493797938272fdf0e7d0",
      "1dd4de68fb96421a9db71d4b6b92183d",
      "5eeec5697c3c4d57a75c1de0befb2829",
      "36a49a6d8bcd45afbbee7949e65f8d89",
      "93fe7b210faf45a79b6343db41ea8810",
      "1b9a51eea989400b9dcd30fc71b4a05e",
      "2568ab866e9c4a7f8ea8e0e194cdb30c",
      "00e900238e2c40c0aabb5e2fffca7a2d",
      "bd122c4324c348489bdc0b37617aa3f0",
      "02a2fa3b5d8c4082a65eb8a99bdb1bef",
      "1eed9c7c7da24ac2b28b6471d6f17de9",
      "8a70d49f601545cc9b23e9a6ff865371",
      "f78a63cfde984b28a098b6fb5325d29d",
      "4cbf8a02c6b34830ad9c21039bb35c7c",
      "fd730acf242e454582017bbb11ffadd8",
      "c660968629b54d98a452d8912b8112d7",
      "e6479c20cedf412c8607c96559800a41",
      "974d7035654a48369e209323c46cc85c",
      "45db2e93a3b64593aff93b85295f0040",
      "d3b119f2ca5b47c9a735f0da30965e41",
      "a4f754d8cc3345bea22c7d1328a80940",
      "66f4d6c029984d51b4d2661d3c9f4502",
      "599b2d2d96d7439dad5a9f156ceed7fc",
      "61ff63910ba5410bae60ae488f47c717",
      "65db8a4ed70842bb8f615c0f95dd88f4",
      "b3cb5eb9cb3544ffbf8daf03e022c577",
      "122afed1c18247f79be80e24dbf60728",
      "26756ba97f314640920da30a73897ee3",
      "033cc0067eb84b7c82a207204b03d311",
      "a03c5f4c95674721bea17a3a3352b9de",
      "0600541103f4479abd973c4c2466e08f",
      "b5ed7cb68eb743b2b85f58aeec75d74f",
      "cfa42b0789a443e2a55277e87034a66f",
      "27c426981b904bf7bc1b112ff0fb3885",
      "54a0772e686c4b7a930b682d8889b1ea",
      "468167d099614d14952e642ab35a54d1",
      "01b674125d8b49cfbfa7bcad0edf06af",
      "467fcde1bddc454bbaf92ff7763a01e6",
      "958e0b6601474e9fa8cfae197a38cc17",
      "92c813cbba78440480c93784daf16110",
      "5a065931f93c42b4937ba97860d58a10",
      "e41861dcceb34bcc95c49b2e092757a0",
      "98edd67b36c2435fb3748def400a6cbc",
      "11ed0adda5fa468fae11de13d0bc25ee",
      "8fb0a5c346374888b2aa25e2f693cd8d",
      "4d2f739e457144c4999c2ba443c5f2fd",
      "285ccd6d86a7414e8d4393d92c503519",
      "6c6ca765c7ee4df7995420ded2e5b6eb",
      "7e4d6bd5b83b4a51a051f5f99babcc43",
      "2d43035d807f433b8c9458a1f06c9e1e",
      "acddce4780de4f8dbd436a369c0f7e86",
      "4a0e54d87ec34f8aac1cb0fd26538752",
      "276543b74353429aa93612af1824b0be",
      "0e974da676ef4a64b9611d8186a47b89",
      "8a6b4e51e55f41e2959aa9ca6483d316",
      "14548fc017884c6ab7949e1ecee345da",
      "2ef64ce7639b4ed3b7bf274b553574e2",
      "f6bca82bd00441f39f35f670f0a64e1a"
     ]
    },
    "id": "WqNQsSxK-eqP",
    "outputId": "04521e53-d8fa-49d3-a194-acaf44ca1f33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e192eb78718d4aee8ba8f5a405f920f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f63cde54b064c848b96a9b8c54e3abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e81e22f5944d198502319a4b833cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575689c8893241419c5a6c40fc2737e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285da956dda549caabb51e01bb8ab903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68bece4bd36416ebb08be72d9f0c3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94c23e4ff084066bcfb528339194ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372c8598981344888fdbde759447ad26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c344bc4cd3047388ee02842fb503ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f652fc455e46416081d5938d65c3885e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a49a6d8bcd45afbbee7949e65f8d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd730acf242e454582017bbb11ffadd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cb5eb9cb3544ffbf8daf03e022c577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b674125d8b49cfbfa7bcad0edf06af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6ca765c7ee4df7995420ded2e5b6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhanz23\u001b[0m (\u001b[33mkhanz23-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250403_001131-is2gbdnq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/is2gbdnq' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/is2gbdnq' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface/runs/is2gbdnq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 07:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.674500</td>\n",
       "      <td>0.624242</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>0.626149</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>0.631133</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.647400</td>\n",
       "      <td>0.628794</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.625536</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6242417097091675, 'eval_accuracy': 0.6838235294117647, 'eval_runtime': 2.7218, 'eval_samples_per_second': 149.902, 'eval_steps_per_second': 9.553, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to apply pruning to the model\n",
    "def apply_pruning(model, pruning_percentage=0.2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Don't prune bias terms\n",
    "            weight = param.data\n",
    "            num_elements = weight.numel()\n",
    "            num_pruned = int(pruning_percentage * num_elements)\n",
    "            flattened = weight.view(-1)\n",
    "            _, indices = torch.topk(flattened.abs(), num_pruned, largest=False)\n",
    "            flattened[indices] = 0\n",
    "            weight.copy_(flattened.view(weight.size()))\n",
    "\n",
    "# Load BERT tokenizer and MRPC dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Apply pruning to the model\n",
    "apply_pruning(model, pruning_percentage=0.2)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)  # Get the index of max logit\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-4,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1PsAguCxmeK"
   },
   "source": [
    "**Bitfit on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "SSy_zeLx7vN7",
    "outputId": "77ea7400-d23e-4d4b-9e63-7e254d3ec962"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 05:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.584993</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>0.517563</td>\n",
       "      <td>0.740196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>0.502057</td>\n",
       "      <td>0.779412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>0.484994</td>\n",
       "      <td>0.786765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.481838</td>\n",
       "      <td>0.801471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.48183783888816833, 'eval_accuracy': 0.8014705882352942, 'eval_runtime': 2.846, 'eval_samples_per_second': 143.36, 'eval_steps_per_second': 9.136, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load BERT tokenizer and MRPC dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Function to freeze all layers except bias terms\n",
    "def apply_bitfit(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Freeze all non-bias parameters\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Apply BitFit\n",
    "apply_bitfit(model)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)  # Get the index of max logit\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-4,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC9vnJWnycJ0"
   },
   "source": [
    "**LoRa on BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "guJrYI3B72cE",
    "outputId": "87e1c409-a1d7-416e-842a-197169007e74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 06:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.543621</td>\n",
       "      <td>0.725490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.489193</td>\n",
       "      <td>0.767157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.526095</td>\n",
       "      <td>0.740196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.571526</td>\n",
       "      <td>0.752451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.577739</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.48919257521629333, 'eval_accuracy': 0.7671568627450981, 'eval_runtime': 2.7949, 'eval_samples_per_second': 145.982, 'eval_steps_per_second': 9.303, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to apply LoRA (Low-Rank Adaptation)\n",
    "def apply_lora(model, rank=8):\n",
    "    # Add low-rank adapters to each transformer layer (example for Bert model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            module.weight.requires_grad = False  # Freeze weights\n",
    "            # Add a low-rank decomposition (A * B)\n",
    "            adapter_a = torch.nn.Parameter(torch.randn(module.in_features, rank))\n",
    "            adapter_b = torch.nn.Parameter(torch.randn(rank, module.out_features))\n",
    "            module.register_parameter(\"adapter_a\", adapter_a)\n",
    "            module.register_parameter(\"adapter_b\", adapter_b)\n",
    "\n",
    "# Load BERT tokenizer and MRPC dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Apply LoRA\n",
    "apply_lora(model, rank=8)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)  # Get the index of max logit\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-4,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdumtZXdysvH"
   },
   "source": [
    "**Full-Finetuning of BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "_0gy2WhP9quF",
    "outputId": "69a2e154-2328-4130-c090-c2a47659b5d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 08:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.670100</td>\n",
       "      <td>0.626005</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.628700</td>\n",
       "      <td>0.626237</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.628100</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.649500</td>\n",
       "      <td>0.625117</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.622400</td>\n",
       "      <td>0.624552</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.624552309513092, 'eval_accuracy': 0.6838235294117647, 'eval_runtime': 2.709, 'eval_samples_per_second': 150.611, 'eval_steps_per_second': 9.598, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load BERT tokenizer and MRPC dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)  # Get the index of max logit\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-4,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                          # The model to be trained\n",
    "    args=training_args,                   # Training arguments\n",
    "    train_dataset=tokenized_datasets['train'],         # Training dataset\n",
    "    eval_dataset=tokenized_datasets['validation'],     # Validation dataset\n",
    "    compute_metrics=compute_metrics      # Metrics for evaluation\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()  # Start training\n",
    "trainer.save_model('./final_model')  # Save the trained model\n",
    "results = trainer.evaluate()  # Evaluate the model on validation set\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYBHCBvGGU7-"
   },
   "source": [
    "# Using BERT on RTE_Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmYyAMuSy83v"
   },
   "source": [
    "**Diff-Pruning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "EaV9Cm0RLqlM",
    "outputId": "80bd056e-5b50-499e-b7e1-1085a03e790d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [936/936 06:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.705600</td>\n",
       "      <td>0.698885</td>\n",
       "      <td>0.472924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692500</td>\n",
       "      <td>0.695909</td>\n",
       "      <td>0.472924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.635900</td>\n",
       "      <td>0.687250</td>\n",
       "      <td>0.548736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6872498989105225, 'eval_accuracy': 0.5487364620938628, 'eval_runtime': 3.9468, 'eval_samples_per_second': 70.183, 'eval_steps_per_second': 8.868, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Diff pruning is a parameter-efficient transfer learning method, applying sparsity.\n",
    "# (This is a simplified conceptual demonstration; real diff pruning would require complex reparameterization and sparsity constraints.)\n",
    "\n",
    "# Load BERT tokenizer and RTE dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"rte\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Apply a simplified conceptual form of diff pruning (sparse fine-tuning using a minimal set of weights)\n",
    "def apply_pruning(model, pruning_percentage=0.2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Don't prune bias terms\n",
    "            weight = param.data\n",
    "            num_elements = weight.numel()\n",
    "            num_pruned = int(pruning_percentage * num_elements)\n",
    "            flattened = weight.view(-1)\n",
    "            _, indices = torch.topk(flattened.abs(), num_pruned, largest=False)\n",
    "            flattened[indices] = 0\n",
    "            weight.copy_(flattened.view(weight.size()))\n",
    "\n",
    "apply_pruning(model)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1Txabm1zftu"
   },
   "source": [
    "**BitFit on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "rLbzxPDaGfUc",
    "outputId": "440cb0a7-018e-4049-a641-3fa4db81d6da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhanz23\u001b[0m (\u001b[33mkhanz23-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250504_171841-1latwi6u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/1latwi6u' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/1latwi6u' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface/runs/1latwi6u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [936/936 04:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.692698</td>\n",
       "      <td>0.498195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697900</td>\n",
       "      <td>0.691547</td>\n",
       "      <td>0.505415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>0.691846</td>\n",
       "      <td>0.494585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6915468573570251, 'eval_accuracy': 0.5054151624548736, 'eval_runtime': 3.8898, 'eval_samples_per_second': 71.212, 'eval_steps_per_second': 8.998, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load BERT tokenizer and RTE dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"rte\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Function to freeze all layers except bias terms\n",
    "def apply_bitfit(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Freeze all non-bias parameters\n",
    "            param.requires_grad = False\n",
    "\n",
    "apply_bitfit(model)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QsEbKSWzk57"
   },
   "source": [
    "**LoRa on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "9OInjYdkGh_g",
    "outputId": "0e2af9de-1597-4e9f-fd67-8ee5d05e36f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 04:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695400</td>\n",
       "      <td>0.695964</td>\n",
       "      <td>0.509025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>0.693289</td>\n",
       "      <td>0.516245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.693342</td>\n",
       "      <td>0.501805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6932888627052307, 'eval_accuracy': 0.516245487364621, 'eval_runtime': 3.6242, 'eval_samples_per_second': 76.43, 'eval_steps_per_second': 4.967, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to apply LoRA (Low-Rank Adaptation)\n",
    "def apply_lora(model, rank=8):\n",
    "    # Add low-rank adapters to each transformer layer (example for Bert model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            module.weight.requires_grad = False  # Freeze weights\n",
    "            # Add a low-rank decomposition (A * B)\n",
    "            adapter_a = torch.nn.Parameter(torch.randn(module.in_features, rank))\n",
    "            adapter_b = torch.nn.Parameter(torch.randn(rank, module.out_features))\n",
    "            module.register_parameter(\"adapter_a\", adapter_a)\n",
    "            module.register_parameter(\"adapter_b\", adapter_b)\n",
    "\n",
    "# Load BERT tokenizer and RTE dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"rte\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Apply LoRA\n",
    "apply_lora(model, rank=8)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJQJ_liszqUa"
   },
   "source": [
    "**Full-Finetuning of BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "edcBDfA_GkB1",
    "outputId": "419d385e-0e0e-43b4-9381-34a1f0617ff1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [936/936 06:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.690002</td>\n",
       "      <td>0.519856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.649232</td>\n",
       "      <td>0.610108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.613718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6492316722869873, 'eval_accuracy': 0.6101083032490975, 'eval_runtime': 3.8708, 'eval_samples_per_second': 71.561, 'eval_steps_per_second': 9.042, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load BERT tokenizer and RTE dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"rte\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfZVaTDGOnZ_"
   },
   "source": [
    "# Using BERT on CONLL_2003 Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53HWxkMxz78x"
   },
   "source": [
    "**Diff-Pruning on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "UUERcHb-Glum",
    "outputId": "6980f48d-eef6-41d8-e390-399ef3ef42bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 128, Min length: 128, Average length: 128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhanz23\u001b[0m (\u001b[33mkhanz23-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250505_005233-ac9euzo9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/ac9euzo9' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/ac9euzo9' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface/runs/ac9euzo9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 18:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.079220</td>\n",
       "      <td>0.978575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.078015</td>\n",
       "      <td>0.981419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.075885</td>\n",
       "      <td>0.983562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='407' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [407/407 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.0758848488330841, 'eval_accuracy': 0.983561537113141, 'eval_runtime': 24.4558, 'eval_samples_per_second': 132.893, 'eval_steps_per_second': 16.642, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the FAST tokenizer (note the use of AutoTokenizer and specifying use_fast=True)\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# Define a function to align labels with tokenized words\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100\n",
    "            # so they are automatically ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to -100\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization with aligned labels\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Checking the distribution of sequence lengths in the train set\n",
    "import numpy as np\n",
    "lengths = [len(input_ids) for input_ids in tokenized_datasets['train']['input_ids']]\n",
    "print(f\"Max length: {np.max(lengths)}, Min length: {np.min(lengths)}, Average length: {np.mean(lengths)}\")\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Initialize model\n",
    "from transformers import BertForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load the BERT model with PyTorch weights (the default should already be PyTorch weights)\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=9)\n",
    "\n",
    "# Apply a simplified conceptual form of diff pruning (sparse fine-tuning using a minimal set of weights)\n",
    "def apply_pruning(model, pruning_percentage=0.2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Don't prune bias terms\n",
    "            weight = param.data\n",
    "            num_elements = weight.numel()\n",
    "            num_pruned = int(pruning_percentage * num_elements)\n",
    "            flattened = weight.view(-1)\n",
    "            _, indices = torch.topk(flattened.abs(), num_pruned, largest=False)\n",
    "            flattened[indices] = 0\n",
    "            weight.copy_(flattened.view(weight.size()))\n",
    "\n",
    "apply_pruning(model)\n",
    "\n",
    "# Define evaluation metrics - use a proper metric for token classification\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.tensor(predictions).argmax(dim=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to 1D arrays for metric calculation\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "\n",
    "    # Flatten the predictions and labels\n",
    "    flattened_preds = [p for preds in true_predictions for p in preds]\n",
    "    flattened_labels = [l for labs in true_labels for l in labs]\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(flattened_labels, flattened_preds)\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rEQQWSpz6pW"
   },
   "source": [
    "**BitFit on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "039c73cbca0d49d3bf14261a06795ca9",
      "d13f7d41637f4810910dd125b0cd5fd2",
      "10594755b4724787a8f340baf87c9920",
      "df33d42a7c5b47f38dc26adee9c5261c",
      "229245ce65444681a6a0be88d0e9d1cb",
      "f1255ad3dc654fc0be281dd680ccc48b",
      "4cebcf81c6644486931563b6397c4456",
      "16242fe7e5624d62ab6a3600671a782d",
      "d729b5100b0349c998d7a5f5830f2f28",
      "e45753e039de40d0ae3ff32ea7cc2e9a",
      "3f3b004d9c664e9192f77898e4233eb9",
      "98451e721dc643689bef335f7ae68ee4",
      "40797a4debdd4afab51a88b6aeea9e5c",
      "4bfc11d116bf4d41b9c892531d134a32",
      "05914f78e0d941e79434a98b915c0cb2",
      "6440feafd60e43ada62943eb8858a5ae",
      "e20e81e7bd6440a0a6a38a0acc20fc84",
      "863151235fb14e2abed6da825bf1b1f9",
      "0a51e28a5b0448ea8e38bb1c4b2fb5e8",
      "ef1e97478ee349cabca7a1521bb9b67e",
      "a79c1a97199f4d0092ca41969ee5811d",
      "a9f2e7a08660442c9efaeba3b21ba5f6",
      "6cc3cc0bb36943d59ca4fd7650ed5d29",
      "0965fcbd154d4eaaaf12d4fc7ad684f3",
      "4e9872103fda4ed1b987622fc6057f60",
      "3aecf5c0cad84ee7a621e382ad74f36a",
      "63c9c71e95ac441ea85a40f033d15a00",
      "61c51be1b5cd4380a577b73be422e8f0",
      "291b5c7aa8bc44f69ceea13d37cb20e2",
      "5ca59120ef9a4f049d5277c9d1288b94",
      "3453ddb665064ed2a4a76c821f0ef85d",
      "fd07d7b560c147b991ca762e475b796a",
      "f599d04c15d14723bfc1c0f26d615f5e",
      "e8089b32e95245b7ba954e80cab87f99",
      "2ced8674d1c84041b6166604708037b0",
      "227582c2d4294a8e91b13975dbba5c37",
      "1efa04824c1d4eb2bca946bc53b121f6",
      "e9a22117f21e46209585fb95ac9c8aff",
      "0509bd3f7ad146b3908f0807cd3af54a",
      "10bca497a856430d8328c582fd3c7409",
      "2af40ad9d30e4be28054a2ec4f4aa595",
      "a9668112399d4141852d3ddabe83d46c",
      "a03446d81fe64f18aec57f1ccd2acece",
      "7d341e25bcbc4b76b24dcb2362b1dc6b",
      "cca4687d03c04e7794962df05593fced",
      "5cf6cd459d8e4af29918a14a2e524bbb",
      "b3c42a62f3454b33b5111fd7bb06dfc5",
      "9be541d18e5949a88c843fbd8f393ed2",
      "efa2fce298e74e51bc797307f5b7d266",
      "e21c1492883d4f4bb25c76109cccdb25",
      "aa8cddd28d1a41dcaac33388e4319e81",
      "980b1c90c1f343058b67dbf25a56c73d",
      "8365d721a3f5487eba3aacc8c7535d3a",
      "02ff8d9254914ba390324cb14b8969d7",
      "dda4ad936fd84a9eb7fb045f46ddb2a0",
      "c8fb65ae79e84c85937f56eac7530871",
      "ea083b7d7268466a81fa0c77225a16c6",
      "f8c0eed68d0c42aa859835c07b10d67f",
      "28b2281cd91e4b2bb3d4eb6d41590ac0",
      "e2106094012f41f6ab61486bd3b7209a",
      "e2ee3fe9410142479930ede66c8297f7",
      "cd5d7a70b3114bf8bc497d713865ae21",
      "85ec793fc7dd4fdf87844e3d8bba969a",
      "fde9585e69a44b61a597f0f60912aa7a",
      "9e309f7f0e6644e7973509ae3eef4a78",
      "e6e1a6d09ebd43a6a458e4ec8b73489e",
      "f285e73de79143a28b62fe753cb78bc7",
      "0008468513234a52a134d95173992328",
      "4c1f0d1dfd0b4d648601ef37bbd34a03",
      "b90bbd7fcc8640d6ac2993816add9160",
      "7aa09182055d4f65805d090cf030187e",
      "b5f5ed98a2b24e0fad7abdfdfb808c0a",
      "4fc7fc6157794622ab6dd677d23200d6",
      "1d4775dc48db4281bfd4a8d14e27e963",
      "9022b4af37a0444ea18575aa90e4e5b3",
      "128166b611da41d984f776df2fe34cfd",
      "019584051afe4fec8ff87049a17a9878",
      "b57d56aa9ae84455ba32fcde8bbb4070",
      "08f7c646466c4759876eede7102eebbc",
      "88cef9cad7ea4455857f827423261868",
      "6d62744f05a144099905f30ec17a4e1e",
      "a3cb4a86f82242f699965ecdfd1b48e2",
      "9971d1cb81444c7596a219729bc4397d",
      "f531a35d886c4002b400ae1a24abf924",
      "e0fcdff5dbea4757b65cfa7c706a26a9",
      "d545fec94af64892ad9ecf434df922e1",
      "dc26efd5ebdb492389063df8d4ae880b",
      "de01688263e047da829b246498de5ee6",
      "6c353303b5574aec82c7b873711b9bcd",
      "5054856eca244f3a843eea738064c86d",
      "fabbf67fa65c4a97ba6c86171897fb04",
      "a05b913870214bcc9baa5d99c46ba1b2",
      "e8a65fc7b06b49099d7a5326f441d778",
      "df2d4da9a8664a8894acde0dd624e24d",
      "42f234a872b946b589c0a4dfc5e2bc39",
      "cbd64a47dd2445569edac3837a3d4c70",
      "ae1bb9aa857c418ebfc99c70be82e5d8",
      "5ccdcbebf4934624b68d91b91c7a8a23",
      "fb635e323f6b4e328654f243cda0fcd2",
      "811b43655a6f493ab7242e4a5ae6785d",
      "1a2a96d46b5a413396b87b9d4e5dc8d6",
      "d519edc0a92e4118af373653897f093b",
      "fc97324a49a54e41877f8b73aa20993b",
      "58ba8b21ba774be3b2cf7b13a43547b5",
      "b52923ef2d16462c98dcd49ac380c311",
      "e53c18b7355545709e7528d12e987d02",
      "7f347f46f0d640f2ae39bced7f76fb6c",
      "65d57602ec3048b2b2474cc1e6f32397",
      "3f169f51315540cc81ce306e68a9fa05",
      "5ee30d02301c4d27aecebea7512dc59b",
      "3777919b89c54f548a8f350bb48fbcd2",
      "47fb641a6cb04d77a19eea0e0a63829c",
      "ad2e0ef57b7c4333b1f570cf5b5aac1e",
      "3a62d3e2e0274ecb9a40dd26dd8d34b8",
      "2fd44aa89e4b44b5818cc5c68bfe41d4",
      "321556d93ea54b259b3844ae50b6b438",
      "6e3d84bdb2bc4d198ad96e02fbbc7b4f",
      "836572cfa181408f886cd5ac04f67477",
      "759f31abb56c477198c8d84ea318bce7",
      "fa6502bfdcc54b259ae65e8dd299bbda",
      "dceede94d4f643149ed96891890a706f",
      "2a87743372224cb5ad95abfad0073ce1",
      "93d9ae80f05b454dbd5448a244cf1ee6",
      "398c547921f7406f88e1bfc5f07fd066",
      "b3237040d2784d258a04fd01615a674b",
      "765b316da2b84126b2c34c741073ad9b",
      "9e88a639548e4dff966d3dac20ea624c",
      "8592713ca3a9496eaed54922d4068c03",
      "3ee4e358f74c409c93a6ab6606ce146a",
      "a431fba0b5d74f418567e62044fa38f4",
      "27bfcb787e9642ca8612f4d5119e7dcf",
      "83c0a39f83ae4a34969acd6051f52a08",
      "66c60e43daf44d2783f79bf3c448dab5",
      "53a1ef295eed48c29a6d9bd235c30c20",
      "c3aba7cbb2024bf7a980df762c60c4e4",
      "5b740c8c227f44f6b29240d80b24c9e0",
      "383fa11fcdf84ceb8f38c8275acdb399",
      "54573f6bd27d4c85b6e827ded6eee530",
      "a8a31865e6e7462987b54b8632e4a630",
      "99ff1c1278a44a67880e624579090080",
      "c1e7ee11e71e46e8afb8005c9437bc71",
      "215c4aec45404d2aa4164192c85be89a",
      "9143226498904e31ab60055764495828",
      "28211978d7194bca92973d7f05ccb0bf",
      "6979612b30b1414fb657c0a3a06437b4",
      "49da25b3af184026ae0271ffd9dfec3e",
      "70e671ff4c764626a9227710ae0509d3",
      "2f8a9ae2d9204a5198d594b408f4900e",
      "606d95366a774063b01b489fabeee23c",
      "e9fc1d70ce604dfb9ea1f524093ecbf5",
      "764829b1b7754e1e9640d23c438b4a3c",
      "3d9b6bb6e47043f2af76943f5c6812db",
      "b27c951ceefa4efebef23cf54b7d3851",
      "eb8616a65d2a453085dcb80cbcd71092"
     ]
    },
    "id": "oSRKv6aaOzto",
    "outputId": "f5c22d36-4713-4639-8f62-3b46d0de9d85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039c73cbca0d49d3bf14261a06795ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98451e721dc643689bef335f7ae68ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc3cc0bb36943d59ca4fd7650ed5d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8089b32e95245b7ba954e80cab87f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca4687d03c04e7794962df05593fced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fb65ae79e84c85937f56eac7530871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N] y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f285e73de79143a28b62fe753cb78bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57d56aa9ae84455ba32fcde8bbb4070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c353303b5574aec82c7b873711b9bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811b43655a6f493ab7242e4a5ae6785d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3777919b89c54f548a8f350bb48fbcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a87743372224cb5ad95abfad0073ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c60e43daf44d2783f79bf3c448dab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28211978d7194bca92973d7f05ccb0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhanz23\u001b[0m (\u001b[33mkhanz23-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250405_192630-ed5qv85j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/ed5qv85j' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/ed5qv85j' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface/runs/ed5qv85j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 12:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.554900</td>\n",
       "      <td>0.648933</td>\n",
       "      <td>0.832558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.562200</td>\n",
       "      <td>0.573831</td>\n",
       "      <td>0.832558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.556677</td>\n",
       "      <td>0.832558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='407' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [407/407 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.5566769242286682, 'eval_accuracy': 0.8325575054048263, 'eval_runtime': 25.8683, 'eval_samples_per_second': 125.636, 'eval_steps_per_second': 15.734, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Fast tokenizer for word_ids() support\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# Define a function to align labels with tokenized words\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100\n",
    "            # so they are automatically ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to -100\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization with aligned labels\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch')\n",
    "\n",
    "# Initialize model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=9)\n",
    "\n",
    "# Function to freeze all layers except bias terms (BitFit)\n",
    "def apply_bitfit(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Freeze all non-bias parameters\n",
    "            param.requires_grad = False\n",
    "\n",
    "apply_bitfit(model)\n",
    "\n",
    "# Define evaluation metrics for token classification\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.tensor(predictions).argmax(dim=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to 1D arrays for metric calculation\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "\n",
    "    # Flatten the predictions and labels\n",
    "    flattened_preds = [p for preds in true_predictions for p in preds]\n",
    "    flattened_labels = [l for labs in true_labels for l in labs]\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(flattened_labels, flattened_preds)\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e2656-30NtI"
   },
   "source": [
    "**LoRa on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571,
     "referenced_widgets": [
      "63f6cf92e2f74f028128f63e187d378c",
      "b76fc15c55f44b99a64a7d9c9125fb6a",
      "4456bd9a29ab47a0ba7c19ab63c8bff5",
      "50e3dbea3c744a979f3bd0366fd4f4b2",
      "2bc5196e0b3a4469bf68b8a48312bc0c",
      "2c61a9b7b3594cc194d9124892995b35",
      "22d79977af8c4cd2a1f196a0a1a6eb8b",
      "4127d70f09d242afb4fce838f3738d0b",
      "1bb208095b5e40fb88a3a02e6a857ea7",
      "339a2cbb914142e39c5aa2e9b06209d0",
      "07627ee39e9f4687a4be1bf32103804d"
     ]
    },
    "id": "a0cEPbd_BxuX",
    "outputId": "75fd4d9b-ac5b-4809-be9b-3aacd1b7cf3a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f6cf92e2f74f028128f63e187d378c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhanz23\u001b[0m (\u001b[33mkhanz23-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250505_152313-7l733fw1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/7l733fw1' target=\"_blank\">./results_lora_conll</a></strong> to <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/7l733fw1' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface/runs/7l733fw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 11:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.156779</td>\n",
       "      <td>0.959235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.105670</td>\n",
       "      <td>0.973414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>0.097583</td>\n",
       "      <td>0.974953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='407' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [407/407 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.09758317470550537, 'eval_accuracy': 0.9749527686344779, 'eval_runtime': 25.7874, 'eval_samples_per_second': 126.031, 'eval_steps_per_second': 15.783, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load tokenizer and dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# Tokenize and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Process dataset\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Load base model\n",
    "base_model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.TOKEN_CLS\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.tensor(predictions).argmax(dim=-1)\n",
    "\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "\n",
    "    flat_preds = [p for preds in true_predictions for p in preds]\n",
    "    flat_labels = [l for labs in true_labels for l in labs]\n",
    "\n",
    "    return {'accuracy': accuracy_score(flat_labels, flat_preds)}\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_lora_conll\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=4e-5,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.save_model(\"./final_model_lora_conll\")\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE-Mf6bL0VEO"
   },
   "source": [
    "**Full-Finetuning of BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "9uVOeqEfO4ez",
    "outputId": "7fb87fda-a6ba-4842-899f-a0cd36774ddf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 19:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.194317</td>\n",
       "      <td>0.925178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.154405</td>\n",
       "      <td>0.940657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.139462</td>\n",
       "      <td>0.949165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='407' max='407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [407/407 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.13946153223514557, 'eval_accuracy': 0.949164752151396, 'eval_runtime': 25.9998, 'eval_samples_per_second': 125.001, 'eval_steps_per_second': 15.654, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load BERT tokenizer and CoNLL-2003 dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['tokens'], padding='max_length', truncation=True, max_length=128, is_split_into_words=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Rename 'ner_tags' to 'labels' to align with Hugging Face expectations\n",
    "def preprocess_labels(examples):\n",
    "    examples['labels'] = examples['ner_tags']  # Renaming ner_tags to labels\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(preprocess_labels, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=9)\n",
    "\n",
    "# Define evaluation metrics for token classification\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = torch.tensor(predictions).argmax(dim=-1)  # Convert logits to predictions (class with highest probability)\n",
    "\n",
    "    # Remove ignored index (-100) and convert to 1D arrays for metric calculation\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "\n",
    "    # Flatten the predictions and labels\n",
    "    flattened_preds = [p for preds in true_predictions for p in preds]\n",
    "    flattened_labels = [l for labs in true_labels for l in labs]\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(flattened_labels, flattened_preds)\n",
    "    }\n",
    "\n",
    "# Use DataCollatorForTokenClassification for dynamic padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator  # Adding the data collator for padding\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWytTpDoeOxR"
   },
   "source": [
    "# Using BERT on SSt2 Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-nlGcUQ0fiu"
   },
   "source": [
    "**Diff-Pruning on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ea220e1b375e4428bd9cc1a37ee78607",
      "cd9769430b1a48d99a947780076c295a",
      "6cbf79ef21a149f0b2c6f102ff6457c8",
      "e8bedc846ab9461fbde83178f27f27d8",
      "a1651826a9a04d81974f170d56bd16ac",
      "666c4ba5773c42e39f80afbf8505ec64",
      "cde6e81a581246729f29fcfeaa21f04d",
      "1dd798f0ec27427c9c3ca388492f38d5",
      "938ab3aeebfe4e8b84873c87b7ab5fd6",
      "9440119e4f974557b6417e463d6b7af5",
      "a409906f1cd14c6e8428dadcdc717b68",
      "973e3861ec30495aa574e90af4a65a84",
      "030ab256f00e429489745c1f0d3b2b2e",
      "8cbad434103f498c97127afad4fa73ba",
      "9f02c17690f44da6816db0493289db68",
      "aa299cbad544455295aa0491e4ecd504",
      "d3a755ff570545c987cb7402926f9dea",
      "130d619d51274027818c3d076b502464",
      "87a1b5f07e094722b0cd7dad18dc4a54",
      "b885a182dbfc41e9ae4ec61d6180a585",
      "2020610135e54e00bad32d5e27de9ad4",
      "b0667c0b1b2a4b6ebfc6a395d8e02419",
      "742147a966564f87892172dfbf8a339e",
      "07033b444f5b41e4871364503aa22020",
      "a679bb8e4ba84f9a9d12abded2e9ef09",
      "b3fca881eb72484d9cc070108e43a546",
      "c20792c5e87845a69f0d136bda79a23a",
      "0ca90aeaea86489eaba7abdae8a8890a",
      "3a9b23c51ae94657929ef94c846282e9",
      "bdaef1ed7c0f49b38dba58300393cc7f",
      "42f0d4351bf54366803223dd73f32d3e",
      "b4102cee16074e9ea8d1174e7a5dde54",
      "84a9dca2da134030b9b78c5f120aa7e4",
      "0e63d899912545b9b94d4f44a07b5b39",
      "8d009dc90e7e452bb2c028fedaefda84",
      "35cac2d5a5a04ae4ade6b94be858c7f4",
      "91c85cf6d86040279b253a881a2c4e67",
      "bab79dfae9fd4f38af50d120258eddbc",
      "38c32301816b42b8a24a4085e80a6cc2",
      "f5234927104c40bb9c00029d987c2291",
      "a964884a58c64354a8d43249c19c69c6",
      "ccf1c983ab6140eb925c464594151fca",
      "137ec1af55a2405db4cca42fbfe9c5c6",
      "9ef9fcfd1c6b468094e5ea5a54b2d720",
      "c6784a1b42dd45609ca026ffe8188be8",
      "2d2d3bc3ab9049f5ac0063710a9861cf",
      "624ac7fa721e40e587b61a5523b1b364",
      "6ffa50f123fd4a1b9dc56a7fd8c06825",
      "48214b28fa444d0a8fb044f78e7849a2",
      "ad1a9e2accb64f6f958fc721d1e2d49c",
      "7cf9917c76194a07969e3012c586c57a",
      "0ed50339a8c140d89a980dc5da2c5582",
      "a1efa53978624ba98adf73edfb9fd928",
      "6557a5100a674e37856850fa4acefcbb",
      "ef0b480cc02642868b0167baebddec21",
      "86418f34c7dc4974be52636e281a3e53",
      "f6f0013ead844e53ae449ca101dc2326",
      "df73feb94951438b80ea2c8942a877c2",
      "2f60a4a3a0ff490aa1df118d8e419a2f",
      "c60dcc40563248e2967f5ff02324ebde",
      "722bd46b311d45bf8ce2d07b7b072b81",
      "0cb1aa1199fe4cce941a66a10f1f1d5d",
      "e1ea78503c4c4527a26236d568726828",
      "81639006d19e49bb828f35521b3b6cba",
      "fc52bd52085741c9926180e56d241975",
      "0646f7d383274b319a5b20674b81c840",
      "852ad62c3d5e4a618603c9cb7f2c2adc",
      "7d75cdf986e642fdba05c5d8948559b7",
      "f4fcca75e43b4d4ba28a97aacc0ba423",
      "5d5863450d444f8188d72f998940ddef",
      "8728226f57a74cccbef2702995ae8cde",
      "97757b185da44adbadebbff184ea3f17",
      "a70f914f18e14465a02990947a068349",
      "c53bbd565c6540d89438ae32101509f6",
      "3c527828fd7c498cb63f8b571c834415",
      "315f15433c1c4735b51308ea7975cbff",
      "64fef57034b34443bec17529a6bb9b22",
      "03671621966a463586a6bb8e09410030",
      "c6bd4493b05e4c4e9fd82219eb5867d1",
      "ba870168eb4a4a2f9af80f6236aa64f0",
      "5400ec728e554fe5b75a72a5a352d395",
      "9728796cce37464badcfccc517909709",
      "88ee1b2c55b24ed3a88b219f65b7606c",
      "042c6167e2224cd48c2c98a3a0006161",
      "c97adaaa281b4c8fa329d8613512417e",
      "202587cc417b47d0b96c6b5c67534461",
      "eb8e2151b7724635b589ab1c7a158f51",
      "8ccd900ffa0c4acea77ab5d2c2798b20",
      "4d5847136b3942ae839d2b8ae43ee6bc",
      "22614e01b6c44915a970ff1fdbb11a38",
      "947f5deddfe34f4cb1659bc42008cad5",
      "7fae99efabdc4d988b9e0bb6969d311c",
      "e745b2c315f24df9a7a99503bb920d08",
      "e591b346eca8485aa3313ed38fa7fb9d",
      "b070d41a6e2c422aa7e2928508913e19",
      "cfdac6565d6640119897313aa9b1bf48",
      "2d32870c37d9400fa489432c3405bd3d",
      "5e56bff5b07145829cfd81789d863549",
      "bef07c21e6084a5882da83735992509f",
      "5fa2eceb152e4dea8cedca45f708ca88",
      "1af640250c7548e09cf892d1c3e04f56",
      "ee34fbd7aeed450dbe220a4b39b56595",
      "0a6c370c60c949b580e7090561184c2b",
      "c13ffbca933b437ca8d64c7535aac3a8",
      "b72878f810974e3fb08f583224031c49",
      "7df2a597a5ab4c6a9e70b70072411b24",
      "e2c27b7a45ef417aaa78b6b800a41d29",
      "6997dc6264f040aaa90693b15980dfa7",
      "7db02c9ea6bf437faa1208cf76454e5d",
      "1627900fffa54233adeb398216c392cf",
      "ff1d078b318c4123a8b707cb99f7347c",
      "c6d1b3dfa073496c916ee9c0119f4f28",
      "8ccdb10c001645b1ad99336c2a10ef60",
      "9dd80a37f6034ccb91f1c3f7809b7e67",
      "f3bd974753a64666b380b8cafa6d1563",
      "5d20e3d2623b4a4ca984470c983057d1",
      "b104f59c8ec2455794ce95545719c43d",
      "93b7a7e830f444a9a9c64ae9c14e5d6c",
      "4e201fbbf8324596b6d8f21dc69d541e",
      "46fe2ae0827f49938a98ab9075daf5ea",
      "dd4c2007e2d743bcafb1cc5159ac31e0",
      "e0eeda6f366a41f2b274f76cb9ccde99",
      "b3fc1466b19b465783348bcfa5cf7b70",
      "bc24f8c09b574bc1b69c2eb1ad2309df",
      "626111c4f0a346eda4517688d69b2e52",
      "984c13d1f7974b449b01993cc3375533",
      "35ad3eaed67245aea3e7809f47240192",
      "4f225bb1b198401f992eff407cc3b27d",
      "4f6d02a778aa48ed8a121c657024ee35",
      "82295a553c5446d3b6456f449dcef572",
      "f58f2c97b29043939df190673ada4d67",
      "8fc6543522aa41d68c89e5f14fa19c41",
      "c717f0e043e9418182e5f67f09641782",
      "e764e00cbc404014b2891100d81e4473",
      "251fa15cd32c4501bbb7fa188bf70caf",
      "711d4c914add458c965000a2eb330782",
      "7e5180fb7bfa4464be6e2319288e1068",
      "de63212e9ec64b01b8b5355407ad1e7b",
      "a04913c5ec454db7984b22e7d5c7dd62",
      "c006239d1ac248bf8db82e401e0a458f",
      "3bb43946a2a84a4d9c76f98a5f7ee355",
      "cf4250a4aeab4eeea41660c722283998",
      "6110cd77d7c64ea389af7ce696dbcab0",
      "ecd0b1ec4e5d4bccbe43514c1879e660",
      "a5ac7749a56a41948fdc7a414391317e",
      "3f0669dfe48e47d0a61ef05083ef7fb7",
      "b94431f855db491ab4e5a5b6c79c77ec",
      "2e3d752fff984116ba859716e6554a91",
      "8de859b88371416db35825450c819c62",
      "0ebb113d59284994a0152cc4b6569b85",
      "6eca384d3e7349d1a634c095e74ec91e",
      "4e56ddf79c6a4b999053da3ace75dcec",
      "ac1ad02b3b9441ab8c95d7deb5574d32",
      "dcf9014f661a46a09516c82c5172d8e8",
      "acf343b7a4d340699d949615ce2c36b5",
      "a249ae1cbe7b42fe9c72eb57c7a7e603",
      "5cc27927627d4e34aff281f9a9f5a5c7",
      "8558b06d4fb24e1190cfea98a4b2d4b3",
      "615fbfd4c2a5472aa6a8d3efffd7fe45",
      "6f1cb4d1c51f476aba257d86f61f5e2f",
      "73ef528edf3d4139a90f1bcacd5d7b67",
      "6c00a9a08ab74bea9f895747e3667e15",
      "7d26ae2955cb49939e02d234a4652e1a",
      "8e1cc397acc646b69a07b4a8ecac0bd8",
      "62806423c4124fbf8485f854cd0173b5"
     ]
    },
    "id": "a73QIVPo9hrp",
    "outputId": "89c63098-3b11-402a-fa39-9477a23b3ff1"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea220e1b375e4428bd9cc1a37ee78607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973e3861ec30495aa574e90af4a65a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742147a966564f87892172dfbf8a339e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e63d899912545b9b94d4f44a07b5b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6784a1b42dd45609ca026ffe8188be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86418f34c7dc4974be52636e281a3e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852ad62c3d5e4a618603c9cb7f2c2adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03671621966a463586a6bb8e09410030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5847136b3942ae839d2b8ae43ee6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa2eceb152e4dea8cedca45f708ca88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1d078b318c4123a8b707cb99f7347c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eeda6f366a41f2b274f76cb9ccde99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c717f0e043e9418182e5f67f09641782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd0b1ec4e5d4bccbe43514c1879e660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf343b7a4d340699d949615ce2c36b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhanz23\u001b[0m (\u001b[33mkhanz23-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250417_042056-9wplqvbg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/9wplqvbg' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/9wplqvbg' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface/runs/9wplqvbg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13142' max='25257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13142/25257 42:17 < 38:59, 5.18 it/s, Epoch 1.56/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.459286</td>\n",
       "      <td>0.840596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25257' max='25257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25257/25257 1:21:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.459286</td>\n",
       "      <td>0.840596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.582515</td>\n",
       "      <td>0.838303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.620396</td>\n",
       "      <td>0.846330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.45928576588630676, 'eval_accuracy': 0.8405963302752294, 'eval_runtime': 5.8894, 'eval_samples_per_second': 148.062, 'eval_steps_per_second': 18.508, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load BERT tokenizer and SST-2 dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Apply a simplified conceptual form of diff pruning (sparse fine-tuning using a minimal set of weights)\n",
    "def apply_pruning(model, pruning_percentage=0.2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Don't prune bias terms\n",
    "            weight = param.data\n",
    "            num_elements = weight.numel()\n",
    "            num_pruned = int(pruning_percentage * num_elements)\n",
    "            flattened = weight.view(-1)\n",
    "            _, indices = torch.topk(flattened.abs(), num_pruned, largest=False)\n",
    "            flattened[indices] = 0\n",
    "            weight.copy_(flattened.view(weight.size()))\n",
    "\n",
    "apply_pruning(model)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    save_strategy=\"epoch\",    # Save after each epoch\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWqiFKK90uTa"
   },
   "source": [
    "**BitFit on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "79db5acbb24547bb97b1752f101f74ac",
      "c2e44f3b05334b05b938f5aa0d596748",
      "be858366950549059a6b400501afa471",
      "784bd29e398f44829e12936882e27d64",
      "fc2972a2da3048c28f5bf83155021eb6",
      "e614c8b9084a4903b1ae36f508ad1f0c",
      "7b219bed34b64b39805dea0766291a75",
      "af47aa8c615041a78d4d8620c0912a29",
      "33de0431ac88406eb78613542de5be3d",
      "c833754db8974ffda4dc2c54e451d4d9",
      "975624e8f6f54706a6494f8ab4ed0f42",
      "6e4d93f8055d4ce8b38d08141aed632d",
      "c411ccefc1bc4288a91d628c428d14d4",
      "d470f419e4bf4a268255be47cda79e77",
      "fce369b6d5884c04b5c7156b583e9a61",
      "22590cf7c3cb432b854a5a79608f4b08",
      "a4d396d34fe6472d936f8c021224b314",
      "070dedacd109477f8f01d28102529337",
      "9fef1a46f01846089fc9f9fb94154cc2",
      "bb952d99d7804793833da072f2cc22d6",
      "7b10bca75b21458088e49de56df971e1",
      "99e1c87338c1498d8e27b78fa74a42cc",
      "558894a9a1a743bb93628aab2cb89bf3",
      "78dbdfbb57f74c5494b8a765a70e00ec",
      "9f3e661f530244a1bbc74fb600b385ad",
      "bd61c90d39904a96b25f1cd888f57824",
      "ba2e10c3eff246949edc5d4946e51424",
      "5e9d4c46a6684d2987a69cc849abde3d",
      "66c91c048f064a16a50db57e8db34f19",
      "b4bdd758bb3b495a86c207c883fb0c92",
      "4e197825f788440d97ed28f78d1ffe06",
      "e879f6070873492dbdebfd46f4d283ae",
      "6b7a80dce69b4e629a5752da55ad0a9c",
      "6a8ed378ee6443a8a61f97422c097913",
      "723704b5b1844c60b327bb18160552d7",
      "bb29bfc11986485f883788e310fb6379",
      "28fa19f39ba2488795dfee58eb9a12a0",
      "ebe8ef983b0c42f9904d604c8e1a7328",
      "65fe792f284e4453a632baafde616d7d",
      "073660a2c291406382f959e493407c42",
      "dd073d80f3ab46ca948bb50bb231ec20",
      "959090fa31aa4f7c8cdae6d410f63c21",
      "c8b4faf69ca247939bc8ec4019603809",
      "ce79eb2f1fc146d0973e27da50fea76b",
      "a738b0d76acb4ceea6645b3c1636dbb1",
      "01e0d175813340bcb2bbca0933d8c58f",
      "a0625598688747a0af46de1783263c96",
      "1fda0515f9d14524841b7929cf9884b8",
      "ce2e9f80ae874e1993d5ef5be91502f8",
      "63005e0141cf4b5182eb3e6801c56eef",
      "e328a4e13bd74d048da3187b17d93b29",
      "55558b3b516b46ce9afa7605741deb1a",
      "62a7098aa8d84dc4a38cbbb08613e9bf",
      "8493cc01f35b4e2a80656796c314cc94",
      "1b4e641c37bf4847a89185c41bf628fc",
      "73e8ee2a2dbe4aa3829e616d7306eab9",
      "17d444c4a5f24709aaab0c70a44e4478",
      "1d651d8e214f48168bfd8160374f5d10",
      "b62709ed872e43be8a2cba3d9128307c",
      "d12910bc2c1f4dfcab371fab22764ff4",
      "6cb191ffbfd54c1bb85cfae0067268ca",
      "1fc862130f6f408197e34d1286f30dde",
      "188ef32a05bf4ffe8d30522fb0c043b1",
      "7a525627fd754fe3b25190ef0ffc2bdb",
      "815277e7793a4fe9bb1ebf3f582f4b39",
      "7ba23fe8bfd8413ea40c948346592ccf",
      "87426d5c4cbe465cb69fe49ba7ef364a",
      "70480044afb4410599d6e2364f0daf72",
      "cf61e041789840daa2c83cc6e6d5a4d5",
      "62db3204e3ef43b8afff072df06ed4ed",
      "d5ef97e73a3f45c1827d7dde4d7f1add",
      "a6e05824424d46d79d2d8235610dbcfc",
      "86cb942e021a450883b6bade9b51c55a",
      "ad65839c20914e878dce054a1df18056",
      "ed6ce44a24bd42b4ac2525f4328d02d6",
      "217eb9e7015b47588d45ad29c3a49a37",
      "5711e9d62eb1421694ed4051ab865403",
      "dc8fef2a3b2e4b99a02b2ae80c9895ba",
      "c54fc5ae78dd45f6b8c64994fb6ce675",
      "307c81194dab49e0a3c4756717173b08",
      "253400a127804ef1a67128c8764ac372",
      "2d9ba8900ca34e3c9c982375f2b9ca6a",
      "300f28ea07614cc98531b60bdb0614d8",
      "4d43d4910a704d769dec12555ea307d0",
      "06010ea19a9f4b11a918bf7ffee93caa",
      "469b94976ec9424fa50073afcc02c394",
      "47850597304443c5af88132c222267ad",
      "3267ece8b5894e948c9979f6e2014004",
      "4c4f3e9028c9428690e6114f60c806b0",
      "cc9084cf2e9a48f9998edb5f2f41197d",
      "e1882df09709417fb304be5aabfa174b",
      "496d3caf5e4e4476aa238e729574634e",
      "2e7d26514e274aeda02d3142e71720cc",
      "19a6bc30d4db4916b8a3c5bb18fe37c3",
      "573558c044b14ae9b00bd9e7746d3e31",
      "62f024d506a343e7b128db64a1aa9071",
      "7c4971a096524ad6b4f14d49086a5a1d",
      "da0cd2138c9944498797d4f256d9c301",
      "aeec76955c174fd7ae73249221711589",
      "783416d2a23c496f81b00437e4fd7c46",
      "129ab88f6b0e4efba08413c98c4105aa",
      "1d97ecda2c43479d8045e5cfaf274146",
      "cda69e802d6848e8886db7331e33b002",
      "dbaf568ba7fd4adf8d1ab3fcad620715",
      "682ac72f5be74208be7898b2437ade04",
      "623acc229afe44e1b3423f83e49c985e",
      "2758be0c96cc4f2581e5c3bcb7b019b3",
      "f2d7c62db6d9436783319f50b377ad53",
      "a612a1937c454e4192cc995536995053",
      "a4d9d910b22a4d10b6099cf50c26ab8a",
      "cbf1eab1171142a4b2236d02191cf1d5",
      "537469a946a14fa4a03ffb4e1a0c922b",
      "bfb6989dae30479ca23ba2a4315b059f",
      "a8c7c0338af74e6aac19b4601d299677",
      "eeb1e92b3d944987805ea708eced87ba",
      "cea82c609049429199459247c7f13f70",
      "bc43b038079a4dc084cf6281386523d7",
      "ae73efbd3a9948f8b9b7b84cc6fd715c",
      "391a89872bd84523b28fce57df4f772a",
      "ae9075539fa64518b2af05511e006c1c",
      "370ec8b08a5b49be8e638c288fbc688a",
      "069e38599f7c42539b86e6f6c3c30366",
      "7cd911ee41ff4a158bcce077e3a91544",
      "96d35ba1c2c24afb8cf41f2636dee028",
      "b57f4619bc05435d95a7266702f5400d",
      "68f6c50074af4d3f975781f9618c626f",
      "ae3e76ddce4841698dac8e86211c7342",
      "3770b2e1b7e54346ad479eb898397ea1",
      "a9705b40177947efa503a519f5a00943",
      "377d056db78840dcbe6794a703a216c4",
      "6f18e71f05514d90a5f245c2b80740e6",
      "6046fd4fcf2e47478d7e030661fb674f",
      "f5810d7a7f2340fdb159758fadb688a3",
      "c543437106ad4bcb9813a4762429ba2f",
      "4df3db8c250945089b8fe8d45be31f9e",
      "8e920c6785f14cb098b551886e30031a",
      "6f3b51ae2887467c811a1f0340a69aab",
      "fbcc1f1377094292948b8e494243917c",
      "ff24d07e16a84142b6e4d5696aab7b61",
      "425794bf79694f8f87b407ad2ace51a9",
      "89af8a83f0dc4f4d9ef284f239ab59e0",
      "23822117fc214af8a9ce765cdf5ddb90",
      "08dfb3808ecb4270a24d33ee95ed7303",
      "317c35ab1b854723b4399b19620e3050",
      "0ac80f806d0f4736b3fc21b894146fea",
      "3a1482d1f0334b8ba4e69a2eef973ac8",
      "628edcf5d6dd4c4dbee0ca01b40c84a6",
      "872721a62276427ba934ef6f44d43c08",
      "2d4da7e33eae407dadc9536b6afdd845",
      "e9670f9eaf224311a5ccee4a0c20836d",
      "5da357be244e4d6ea42f4283ec795de5",
      "3747203005e24eef9d94e076aaf69a46",
      "07319e7c34654901bd0291e0130f05cb",
      "ffb9a87e08b8446980ecfdf91fe4a69a",
      "31d8ebe9f6954f10accda9b81fa5fc39",
      "8754b4f6ac82491d90e2bf55424a5eb7",
      "5e9ba44022844fa5b765d3c0a3e3650c",
      "6ff90bdf7be64a758fc09456b91e62d3",
      "523d2b32968043c487f64116a098ec22",
      "4ea794ad2c6344c7a5511247bf86eba6",
      "2f7a1e90e1114c879fcd30d664804135",
      "572bcc151dc04b4fa80d8945eddf9df1",
      "6ebc7442aa854837a7baa3538eb83cbe",
      "0554febf31214cd29f4908666fe63eda",
      "d901afc4809842c79b2357ed4d01ba66"
     ]
    },
    "id": "UUpLrBwHeXaS",
    "outputId": "473919e2-7a6e-4dca-d6cc-1c908aa0028f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79db5acbb24547bb97b1752f101f74ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4d93f8055d4ce8b38d08141aed632d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558894a9a1a743bb93628aab2cb89bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8ed378ee6443a8a61f97422c097913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a738b0d76acb4ceea6645b3c1636dbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e8ee2a2dbe4aa3829e616d7306eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87426d5c4cbe465cb69fe49ba7ef364a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8fef2a3b2e4b99a02b2ae80c9895ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4f3e9028c9428690e6114f60c806b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783416d2a23c496f81b00437e4fd7c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf1eab1171142a4b2236d02191cf1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069e38599f7c42539b86e6f6c3c30366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5810d7a7f2340fdb159758fadb688a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317c35ab1b854723b4399b19620e3050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d8ebe9f6954f10accda9b81fa5fc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m88rehaan88\u001b[0m (\u001b[33m88rehaan88-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250417_191954-s9sryltx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/88rehaan88-cardiff-university/huggingface/runs/s9sryltx' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/88rehaan88-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/88rehaan88-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/88rehaan88-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/88rehaan88-cardiff-university/huggingface/runs/s9sryltx' target=\"_blank\">https://wandb.ai/88rehaan88-cardiff-university/huggingface/runs/s9sryltx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25257' max='25257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25257/25257 48:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>0.358180</td>\n",
       "      <td>0.857798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>0.320404</td>\n",
       "      <td>0.881881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.298400</td>\n",
       "      <td>0.316987</td>\n",
       "      <td>0.876147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.316987007856369, 'eval_accuracy': 0.8761467889908257, 'eval_runtime': 5.8486, 'eval_samples_per_second': 149.095, 'eval_steps_per_second': 18.637, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load BERT tokenizer and SST-2 dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Function to freeze all layers except bias terms (BitFit)\n",
    "def apply_bitfit(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Freeze all non-bias parameters\n",
    "            param.requires_grad = False\n",
    "\n",
    "apply_bitfit(model)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    save_strategy=\"epoch\",    # Save after each epoch\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mViZ92wX00q0"
   },
   "source": [
    "**LoRa on BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581,
     "referenced_widgets": [
      "cacc4529229f41529af2624175ef5afa",
      "76cf3afa71b94491b5e35832f7672800",
      "e7bb5aef9f8144fbb41a95a398d9c93a",
      "b87db156f07549e6bfdc838ab2a2e40c",
      "19308cc3f3a74570b31cd284fda6ebfd",
      "ffb7c2aa02374ef6bb59bcf4dd7d168d",
      "98980f9526fa4fa69a58a90459289a32",
      "29a45d8dcbcb4d71a0fb2c23dc1bfb27",
      "2d41ced54a184d21af4cd28fe3972328",
      "35544e34ec4d43f19031f8a6d5709902",
      "7c53d343d41548b888a34d48569ccb00",
      "8c2772deedaf48ac932b921fd5b3536c",
      "66dc09f4ebde4c268e75ecbc543e14d0",
      "b560bd0483bd4affb0b1ef49be5d5081",
      "99bbb20e798f4fdeb6731441f98bf8ab",
      "e1641184986046fdba911a900d632165",
      "5ffde09aa8844a9b9819395abf1d073a",
      "ca38f49ab0fb454096c42c0727bc4a8d",
      "ecb334f8965c4d4d8ce16eccc247adda",
      "5027d320ca464da4aa09b432e62f008a",
      "d5e2e0fc96a4433ca5fd2da92cd50c38",
      "40420a4f6f5b4fbeae5b2794c228f520",
      "4a75505f6bee4edfaf9154c41d370e63",
      "ac8a656d5ade48058399ede19d715bef",
      "26ffcd53ad3a4349829783e9af2c646c",
      "eaaafa8bc3d447c3a481b3bdc323ec4e",
      "c79c082a07f2456a8877f3309b9a9354",
      "55fef042dfe5458d946c73f251e367e1",
      "55cc6daebd234b108bbfa155c6e01763",
      "5fa06d8c55f54024868a74d886a5a51d",
      "a4e817f166714982873520886f6c6f6a",
      "ab5ba2cf56824244aa77a8f88032d556",
      "7a239248247f4e26b0b419462a95a5bd",
      "ede0fd18d7bd48a89d0d11de3b6acd3a",
      "d9a293a548cd443c830e75a6403df3c4",
      "5362b7ff2f70470b96e4652ef2856e12",
      "d8ec705697154cb2a3a918fde0152526",
      "d34c8f0bea814968b41c9be369d6ce02",
      "4ae8951cbb9445f994ce17b8ef0f1cff",
      "c74864da688a427d981157c7527eb125",
      "93cb99a52bee42e1ab98a51b3d63dd53",
      "fa79b3bcf2e34c52b82e4bb71d5c818b",
      "6effeb1ec8f04cfab5e0157d44a3f7d2",
      "e2f4f70fcbb2450295b6157c38ed4a93",
      "4200f91e7d794c979e630a4761c6f4a6",
      "561dcb423724416a90725fff7939340b",
      "938d3064f06846af99d5bf531e7a6622",
      "1aae8d7e29d640e891622f6cb73d54b3",
      "154fd8b7972d4ff6aa2535a7bfbf73a3",
      "fecfa0c59dee4c43822c12332f7825dd",
      "82f8401bbbae4bbc860b0df65f50ccd4",
      "a678e2ab4c844da7ba69d6756d75b31f",
      "e672683500ac4e45b16ca80e99d6b7fb",
      "e99676b9614944acab0d10f17f5aa754",
      "d58a8542d63248ff810702994886ea39",
      "e31dab418d2e4c90ac8ea412709b7c4f",
      "cea68c8ca5834ee19119ab557fdb12d7",
      "2529340d50c74b23b8877e52ba49a98d",
      "b044d2dccbd44b4b8378ff7db814cbd2",
      "7827e3fe2de74c428e50a8a3af172f14",
      "559f65f4aa1c489a80355a24529ca885",
      "ae3569c4ee2648d1894c31f98a972bde",
      "34bd249304d8465fa108291dd83168c0",
      "110c4b1a57124c918cfe589df4378364",
      "0a41d921da054db8b83ee2f68a46cdc8",
      "04567bdfbd394dcab64975140d37c34f",
      "2005b0279eeb4f9e90b97ef8a0eb7e13",
      "c7c6f1f09df14ae9b77ce7e68141e180",
      "994af16990b24c119807ef7df3c715b5",
      "b4156e15dcc44ed7924fa0de683264f6",
      "f93ba0e9c4af42cdadaa223f895b5ee3",
      "a35f7ca4bce7462185f52181bd0ebaee",
      "7c69a2ec34f242cfadecb8da1f235c5f",
      "da0741a17e144ec3a2fc6bbda4f5ddba",
      "786e253adf0041e7894521056685ec85",
      "f4516250cb344497b93b8d63655b1f1f",
      "e410ce6a285a4749b1af523f1a810f25",
      "66aa114a39a141308d03e8673b4f7213",
      "6ea5084d9f72413c9dfe944c11e748e5",
      "c194b82e970e452fad68f21c84ccb43d",
      "739f2d33a90c454dbde052fda5b76557",
      "cbb8a505f6d541f1ba6a79203abe672b",
      "0b202f107bba4393babe3c2133333fbc",
      "a77dcd9db76b42d0b17e1871d11978db",
      "16d40972c44f453898e397066b5c12cd",
      "063789d183db4a128b0622ee3e4f1e7f",
      "9c9e0ea0a63c4820b32e456e82769a50",
      "c0640f754f4840d0958048e7929d5e66",
      "4ec61d86cb3d4a76b5e3747750fb9822",
      "39758f6894ce4b0984d5c6b547edfce5",
      "c744e0e4ac824d9e888cd6d0bd88a465",
      "34ed64a720bc4244962d8ca5cb82e60f",
      "d5fd24ad138848c087bec1de3ab4a00d",
      "cbab1f7c103b4a1a8c92b46ad84531c0",
      "5c8f8058fcc44a68b66393d34d58a5ee",
      "3255196e27944dfb98088d34d3f5ef1c",
      "553380f5b0414acaa3b5d531f6629b28",
      "d61147c95a4b4768a55c6334ea92c4e0",
      "e0bf6f96c3a540b7afbe320f2e7b4eea",
      "5c5feec0ae404cac9fb1aacc83b8c6d2",
      "bd8c171ceef9429f948aa73aace05930",
      "5059192736ed400c9ab82b5fb530aca9",
      "de620e0022bd49f092c5ed2e1ab26409",
      "83f0656d73904ac8b9c95f20fbcfd319",
      "33ffb061ab924a4caf1177d772a1002b",
      "b90a3f335309413fab8aecfdb0ef7fb5",
      "61bb21ca5e194c04b41ed794719160e7",
      "b8b4875a2e5641e8bae70cf53d1a6d29",
      "2470b7fa002b46889b832078e0bf6295",
      "aa8dfba463d34868a24d8ca1385d88ff"
     ]
    },
    "id": "5hUQx5ifeaYN",
    "outputId": "85d2414c-28a3-4a95-f571-e8fbdfc7da5b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacc4529229f41529af2624175ef5afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2772deedaf48ac932b921fd5b3536c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a75505f6bee4edfaf9154c41d370e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede0fd18d7bd48a89d0d11de3b6acd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4200f91e7d794c979e630a4761c6f4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31dab418d2e4c90ac8ea412709b7c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2005b0279eeb4f9e90b97ef8a0eb7e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66aa114a39a141308d03e8673b4f7213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec61d86cb3d4a76b5e3747750fb9822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5feec0ae404cac9fb1aacc83b8c6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25257' max='25257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25257/25257 52:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.358867</td>\n",
       "      <td>0.869266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>0.393124</td>\n",
       "      <td>0.858945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.387853</td>\n",
       "      <td>0.871560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.35886749625205994, 'eval_accuracy': 0.8692660550458715, 'eval_runtime': 6.0446, 'eval_samples_per_second': 144.261, 'eval_steps_per_second': 18.033, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to apply LoRA (Low-Rank Adaptation)\n",
    "def apply_lora(model, rank=8):\n",
    "    # Add low-rank adapters to each transformer layer (example for Bert model)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            module.weight.requires_grad = False  # Freeze weights\n",
    "            # Add a low-rank decomposition (A * B)\n",
    "            adapter_a = torch.nn.Parameter(torch.randn(module.in_features, rank))\n",
    "            adapter_b = torch.nn.Parameter(torch.randn(rank, module.out_features))\n",
    "            module.register_parameter(\"adapter_a\", adapter_a)\n",
    "            module.register_parameter(\"adapter_b\", adapter_b)\n",
    "\n",
    "# Load BERT tokenizer and SST-2 dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Apply LoRA\n",
    "apply_lora(model, rank=8)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    save_strategy=\"epoch\",    # Save after each epoch\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCIHU4px08E5"
   },
   "source": [
    "**Full-Finetuning of BERT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "2fasH66MecZb",
    "outputId": "01f562f6-50cb-4667-e284-504097d92f60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m88rehaan88\u001b[0m (\u001b[33m88rehaan88-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250417_221929-en4nia52</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/88rehaan88-cardiff-university/huggingface/runs/en4nia52' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/88rehaan88-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/88rehaan88-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/88rehaan88-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/88rehaan88-cardiff-university/huggingface/runs/en4nia52' target=\"_blank\">https://wandb.ai/88rehaan88-cardiff-university/huggingface/runs/en4nia52</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25257' max='25257' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25257/25257 1:21:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.450615</td>\n",
       "      <td>0.897936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.248700</td>\n",
       "      <td>0.404286</td>\n",
       "      <td>0.903670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>0.446142</td>\n",
       "      <td>0.905963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.40428608655929565, 'eval_accuracy': 0.9036697247706422, 'eval_runtime': 6.3993, 'eval_samples_per_second': 136.266, 'eval_steps_per_second': 17.033, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load BERT tokenizer and SST-2 dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert logits to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    save_strategy=\"epoch\",    # Save after each epoch\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate the model\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqpAxT7_ejxi"
   },
   "source": [
    "# Using GPT-2 for MRPC Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_QGRvGy1Ji1"
   },
   "source": [
    "**Diff-Pruning on GPT-2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293,
     "referenced_widgets": [
      "6b044414c0b44f7f999ce2c409a3ae8d",
      "83efcd5c7bf444fb9bbc199a3ea0c505",
      "a168484a4277463da1d309a6f9b0067c",
      "59d16a781adc4e77ba47ee2ea6523b19",
      "bf696a0f23a0450e859281ca9a79a9e3",
      "e1a8ac1b673e4f75a01b87e474fe0798",
      "d607cee7fc634572a01b47802082a1e5",
      "20e689c627b745409e37b35a873af9be",
      "a25de1ceccbd4179ad326ce5736f90e6",
      "99a2b1e07c374101a658e03b41635b70",
      "e090d98a10bc429a829c1828ab927fe2"
     ]
    },
    "id": "ktzvfdk2edSZ",
    "outputId": "05404e4d-16be-43b4-fe4d-7438c844dc96"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b044414c0b44f7f999ce2c409a3ae8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 05:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.737800</td>\n",
       "      <td>0.594218</td>\n",
       "      <td>0.700980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651800</td>\n",
       "      <td>0.579735</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.524400</td>\n",
       "      <td>0.588914</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.5797353386878967, 'eval_accuracy': 0.7083333333333334, 'eval_runtime': 3.1568, 'eval_samples_per_second': 129.243, 'eval_steps_per_second': 16.155, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 needs a padding token set explicitly\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Tokenize the dataset (MRPC task)\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Apply Diff Pruning (simplified pruning by zeroing out some weights)\n",
    "def apply_diff_pruning(model, pruning_percentage=0.2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:\n",
    "            weight = param.data\n",
    "            num_elements = weight.numel()\n",
    "            num_pruned = int(pruning_percentage * num_elements)\n",
    "            flattened = weight.view(-1)\n",
    "            _, indices = torch.topk(flattened.abs(), num_pruned, largest=False)\n",
    "            flattened[indices] = 0\n",
    "            weight.copy_(flattened.view(weight.size()))\n",
    "\n",
    "apply_diff_pruning(model)\n",
    "\n",
    "# Define metrics\n",
    "# Define metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert numpy array to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    save_strategy=\"epoch\",    # Save after each epoch\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3fj-gOV1ScC"
   },
   "source": [
    "**BitFit on GPT-2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "TEMDXx79esdo",
    "outputId": "80ffae50-348b-4860-e052-01710b9b3a16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 03:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.771400</td>\n",
       "      <td>0.644808</td>\n",
       "      <td>0.671569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691600</td>\n",
       "      <td>0.618649</td>\n",
       "      <td>0.678922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.642900</td>\n",
       "      <td>0.611239</td>\n",
       "      <td>0.683824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.6112387180328369, 'eval_accuracy': 0.6838235294117647, 'eval_runtime': 3.1395, 'eval_samples_per_second': 129.957, 'eval_steps_per_second': 16.245, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 needs a padding token set explicitly\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Tokenize the dataset (MRPC task)\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Apply BitFit (freeze all parameters except bias terms)\n",
    "def apply_bitfit(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:  # Freeze all non-bias parameters\n",
    "            param.requires_grad = False\n",
    "\n",
    "apply_bitfit(model)\n",
    "\n",
    "# Define metrics\n",
    "# Define metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert numpy array to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    save_strategy=\"epoch\",    # Save after each epoch\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj6sszk81kc8"
   },
   "source": [
    "**LoRa on GPT-2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "-dp_vpi_eu48",
    "outputId": "37fa489d-1f7c-46a0-e66b-1dbed1616738"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443,904 || all params: 124,885,248 || trainable%: 0.3554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 03:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.689100</td>\n",
       "      <td>0.600844</td>\n",
       "      <td>0.691176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.648600</td>\n",
       "      <td>0.574495</td>\n",
       "      <td>0.698529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.563200</td>\n",
       "      <td>0.569803</td>\n",
       "      <td>0.693627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.5698032379150391, 'eval_accuracy': 0.6936274509803921, 'eval_runtime': 3.3067, 'eval_samples_per_second': 123.387, 'eval_steps_per_second': 15.423, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 needs a padding token set explicitly\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Tokenize the dataset (MRPC task)\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification task\n",
    "    inference_mode=False,\n",
    "    r=8,                          # Rank of the update matrices\n",
    "    lora_alpha=32,                # Alpha parameter for LoRA scaling\n",
    "    lora_dropout=0.1,             # Dropout probability for LoRA layers\n",
    "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"]  # Modules to apply LoRA to\n",
    ")\n",
    "\n",
    "# Apply LoRA using PEFT\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # This will show how many parameters are frozen vs trained\n",
    "\n",
    "# Define metrics\n",
    "# Define metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert numpy array to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    save_strategy=\"epoch\",    # Save after each epoch\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize and train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8reTd8tw1uYn"
   },
   "source": [
    "**Full-Finetuning on GPT-2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "h-g6hS2vexGs",
    "outputId": "5bfb7d6f-b2be-4500-d945-75cb21b18c42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 06:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.679400</td>\n",
       "      <td>0.515138</td>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.464433</td>\n",
       "      <td>0.786765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>0.616611</td>\n",
       "      <td>0.786765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.464432954788208, 'eval_accuracy': 0.7867647058823529, 'eval_runtime': 3.117, 'eval_samples_per_second': 130.895, 'eval_steps_per_second': 16.362, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 needs a padding token set explicitly\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Tokenize the dataset (MRPC task)\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    logits = torch.tensor(logits)  # Convert numpy array to tensor\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    learning_rate=4e-5,\n",
    "    save_strategy=\"epoch\",    # Save after each epoch\n",
    "    eval_strategy=\"epoch\",    # Evaluate after each epoch\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "# Initialize and train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model('./final_model')\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "napSV2amYMRv"
   },
   "source": [
    "# Using T5-Small on MRPC Dataset:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IEGxdJK3rM2"
   },
   "source": [
    "**Diff-Pruning on T5-Small:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "_a4nya33RAWz",
    "outputId": "d5cfb80c-ee17-41fb-8475-01043d266602"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-253aea6b6689>:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.086154</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.049894</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.053108</td>\n",
       "      <td>0.821078</td>\n",
       "      <td>0.871252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results: {'eval_loss': 0.04989397153258324, 'eval_accuracy': 0.7941176470588235, 'eval_f1': 0.8536585365853658, 'eval_runtime': 5.4509, 'eval_samples_per_second': 74.85, 'eval_steps_per_second': 9.356, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load MRPC dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"paraphrase: {s1} </s> {s2}\"\n",
    "        for s1, s2 in zip(examples[\"sentence1\"], examples[\"sentence2\"])\n",
    "    ]\n",
    "    targets = [\n",
    "        \"equivalent\" if label == 1 else \"not_equivalent\"\n",
    "        for label in examples[\"label\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(targets, max_length=8, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize and format dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Apply Diff Pruning (zero out smallest weights)\n",
    "def apply_diff_pruning(model, pruning_percentage=0.2):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'bias' not in name:\n",
    "            weight = param.data\n",
    "            flat = weight.view(-1)\n",
    "            num_pruned = int(pruning_percentage * flat.numel())\n",
    "            _, idx = torch.topk(flat.abs(), num_pruned, largest=False)\n",
    "            flat[idx] = 0\n",
    "            param.data = flat.view_as(weight)\n",
    "\n",
    "apply_diff_pruning(model)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results_t5_small_mrpc',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=4e-4,\n",
    "    logging_steps=50,\n",
    "    logging_dir='./logs',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=8,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Convert to binary labels\n",
    "    binary_preds = [1 if pred.strip() == \"equivalent\" else 0 for pred in decoded_preds]\n",
    "    binary_labels = [1 if label.strip() == \"equivalent\" else 0 for label in decoded_labels]\n",
    "\n",
    "    accuracy = accuracy_score(binary_labels, binary_preds)\n",
    "    f1 = f1_score(binary_labels, binary_preds)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.save_model('./t5_small_mrpc_diffpruned')\n",
    "results = trainer.evaluate()\n",
    "print(\"Eval results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So61ggTw32p1"
   },
   "source": [
    "**BitFit on T5-Small:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "PDB7NKOb4XAR",
    "outputId": "06a1c907-4cca-43e7-a443-187c6baa2596"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-24dfc6b55c02>:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 01:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.712700</td>\n",
       "      <td>12.732745</td>\n",
       "      <td>0.316176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12.485400</td>\n",
       "      <td>12.652284</td>\n",
       "      <td>0.316176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12.681400</td>\n",
       "      <td>12.618643</td>\n",
       "      <td>0.316176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results: {'eval_loss': 12.618642807006836, 'eval_accuracy': 0.3161764705882353, 'eval_f1': 0.0, 'eval_runtime': 6.3197, 'eval_samples_per_second': 64.56, 'eval_steps_per_second': 8.07, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "#  Apply BitFit: Freeze all weights except biases\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Load MRPC dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"paraphrase: {s1} </s> {s2}\"\n",
    "        for s1, s2 in zip(examples[\"sentence1\"], examples[\"sentence2\"])\n",
    "    ]\n",
    "    targets = [\n",
    "        \"equivalent\" if label == 1 else \"not_equivalent\"\n",
    "        for label in examples[\"label\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(targets, max_length=8, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize and format dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results_t5_small_mrpc_bitfit',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=4e-4,\n",
    "    logging_steps=50,\n",
    "    logging_dir='./logs',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=8,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    binary_preds = [1 if pred.strip() == \"equivalent\" else 0 for pred in decoded_preds]\n",
    "    binary_labels = [1 if label.strip() == \"equivalent\" else 0 for label in decoded_labels]\n",
    "\n",
    "    accuracy = accuracy_score(binary_labels, binary_preds)\n",
    "    f1 = f1_score(binary_labels, binary_preds)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.save_model('./t5_small_mrpc_bitfit')\n",
    "results = trainer.evaluate()\n",
    "print(\"Eval results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KIpRxgs38xB"
   },
   "source": [
    "**LoRa on T5-Small:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d68a789c878640d09361e8071fc6ad01",
      "e5ec244c5d6b452cb5e376a4df9d554f",
      "1b511ccfd2f04a9aa46d3d6f07ea4efc",
      "dde056d105804bcfa071b2502ee7c843",
      "e438f4dc9d0f4725b9d79040bbe5dc63",
      "c9f23227c4854019bc385fb917e32c0c",
      "27484c13a6a24012a8e627d55e287fba",
      "a6266394c98a493c92cc3f5d5bed2250",
      "d4237480789143e5bace4abca9dab6e1",
      "158cbb680cd54898a3c478e32475f1bf",
      "3e1a435dde314c47852fc0d9be0ad8e8",
      "4ecf79d06acf48c2802c6eec40604919",
      "62cf8b0639694993a46731b603362f8b",
      "8fcc98f08414400287fb6e4bd1bdcd54",
      "a98befd18c9a46beaedfeee4e2a79ff1",
      "52503da9609f45feb9ba4287b0855387",
      "6ea773f8a0194767942e08c374940677",
      "04f90c37ad7d4790b5a06b70b2138c4e",
      "1dd09da67cd94029aa2f3959ae53b675",
      "3af0a65cebcc4b5c82a692be61a382d1",
      "33f69db8a71e492680250e49a48dcd6a",
      "08ffe7dd89c1437fb263527fd016257f",
      "014a84e87f014556975c6f37bf89d7fb",
      "9e633ddcbffa4f2f9fdf60309aa7f1ad",
      "44119423c6e44e58a3022ba0e76945f7",
      "afa76a0f977b4ee19ed3a48de84c5837",
      "735ec9dc42174e2ca872cc1ebf4da28f",
      "349a6d7312874f6b9b45828acfc7187f",
      "e6fab415f4fa4b95bffb3bf7489d0a35",
      "a5ff26cc67a4435abd6adcd0f41d20fc",
      "3ebe8bcf84b2496888d2b02c3315e8cd",
      "e2e14ffe3a94447e958ac953151acf96",
      "ee183d39488c4058a5abd531db8516d6",
      "95bc48725f6c4d83887e76d4988a72e7",
      "99b46339d1934a4da7006c084462c892",
      "27799ee875b245eea4512f71d3817def",
      "b8267db7f8a641cb81581efbe78db003",
      "c0c9b8146ec84b9ab6547829473ab323",
      "b92fcc4d1f4f4651ae7683fd2b64cdc5",
      "d6f236890cf2440784a88f2bcc0434fb",
      "b2b5d9f49d0e483a81fa399697cf29ee",
      "850b4f7852df411ba541bd47a2ea933c",
      "0ef7e0ec34bd4cd3beb0490aa8987437",
      "d7b31f7441f747169a65912773122cb1",
      "459eda8bcf6a48a3a0832a39fdb39208",
      "d6a217da2677440f8df16899cff4de6c",
      "36a29a5592f24801a383ece7edb6273d",
      "b85090c6eb0248798ed90d519e6983bb",
      "57461cae3c5c4f1f80a5ec8f115a62c5",
      "0931cc469fd549cab8129eeec7575110",
      "4541f91ca870403ba73e5e995ea04ecc",
      "19b8d4c77de44232935323c50a47d905",
      "03365cb8107849279a8a0ba4485fe725",
      "77d25ae51f3b466a8682cd07144a55b7",
      "03f3b1c52e9e4e828b8ee18d0a372fbe",
      "b353b56593b04fb7a6df996e512891a7",
      "254ebb6964634c85a87cb7c01cddca82",
      "5fff794399244c519166292900ba8c5a",
      "99dbe00221ec42e795a51084df213921",
      "6764b67f58dc48359e8c34cdeea72c71",
      "53e2dac2a78249dea5cba58b47125134",
      "a0e7bc3a5ba34e71a29b44e03f9ca849",
      "8405019a37eb47159b2880081330066e",
      "6876cc2d4e3740abbe7ee1401f1faf5a",
      "43c88fb8bd154fa89cdb2753963228b4",
      "1defb761ce47497f99b69822f39376be",
      "54ae46b528e54f538c7c3772dae77cbb",
      "c22dd27f44004da0b08e1b307ec98b67",
      "f4ee9f694ea9411eb2cab2a921b0461f",
      "55cae4b0e21d4473935782bbb8b4a3d2",
      "992fa611f9584f4cbf5f37a18e992485",
      "f50553c7b56649c6829db9e97ce105b9",
      "384f493ef3e64e4bb087549d929f8ca6",
      "2b80e720cd7442a697e13e034af4586a",
      "cc652e41554640b38cc8ed5178d928be",
      "b28ff8f54fd946a1b2e53667cfded1e5",
      "3e91495849034a3ba8502b529ae0a03c",
      "c29375ca7b37436c9866a13ed2874fd5",
      "dbb591e4facf45e2af9c3d3f4aace329",
      "7460dab340524fc2ab2a3f5e93a6bfe3",
      "a7b13d07500343a696d47033f9d63fa0",
      "5d854aa975c1461884e6badcf3f4e62d",
      "cd10f493c8984849885b6b37be4101b5",
      "537990cb62994ddca16dcab9c44f034a",
      "cf892e7b81ae4ffd8f370e6e047f9c07",
      "2f5b9da3ef83465c8279161fbf6d93dd",
      "8a87613f259c4dc6b950c80d57c5f3d9",
      "51854c6af5594ad4a7f6f9c3c0c694a8",
      "6913f1c67bf6419c80662094a5e834f4",
      "5116af3a76fb4fddbc32bd78d6c90c36",
      "c87f5111e03f4146aa72775988d0b93d",
      "9b937748772849d5b93a2a73e18402e2",
      "03f8077c553b4af29c65530a560ea337",
      "426566f61bd347e38bb2b9089dafe68a",
      "0d14fbd313944ec486dd1199ff3733d6",
      "735d6c920d1a45179926981c401a100d",
      "b9062582a1d64036adf16e7d920e709b",
      "373f04d986944eb48dceab91292c7561",
      "d808caa153504b1cb9934c975c54bc48",
      "8564511e1b2d4abbbe2d4be38b31d738",
      "03b8ce43f5fa489c980e2cca308a776b",
      "af614ea670864208850e6a2f93b28eae",
      "57c3f7fa70be40639bac27b0de09b2c3",
      "06f81bcfcbb441309744786e827b944c",
      "6202f6b21547411bb858e9385d71571a",
      "e5e4acaf52f9481988ea0c05e897935d",
      "b1a5fa278a3e4823b4cc001cd1dd8621",
      "30fadedc680249b6b88ac3075572481e",
      "d2d585300e2749bcbdef64e2993ad995",
      "eaded09c9b3b49a49c09fbf776f716de",
      "5b069850b5df435e8285a3c49b3df2e8",
      "52bb530fe6114f37a450fe17cc715493",
      "0b47866d260b4bdcb8da895fc331a233",
      "62ec0bad7fa34b60b191120c7273ac03",
      "ad23db4b17014b2c83afa115fe85049e",
      "ea2561c1719d48bca0847f8c415588b1",
      "79e3006e77d9484284b172248efda1a9",
      "03e870fda3b94d5fae575693ee5e4b03",
      "f760ca41841a47389505f6f9ec7eedbc",
      "8b9b0689d9c241ea9c3a90455fb7789d",
      "8483238444f24ee38baabd2dd7500548",
      "cb60501950dd4861950c0706e8cd2ff0",
      "aae785dd1e254c498e9693fe779294dc",
      "5650838a58744d7ca2d95baed38433bd",
      "f897f3f3bfd74fb98986ffabdca0fdcc",
      "03c0bd7a994b4bc5aad491a8a43d7d52",
      "3f2e8be1a3544c009392fecb27ffc8f3",
      "9e35406dee56449697e60a8c0a911bc2",
      "528b23c076b743d78ffa89d46db9da5d",
      "945c7befa47d4b0ab7e642f5a558cab8",
      "525687069af24c0ca61c99f9d70c24a3",
      "96251024d02a402d96f22cece078fbeb",
      "4e9cd4be0ffa402da3b376020153f887",
      "c16d114ff1a74d3399e56dd2626f3706",
      "1817e665a10143f9901eacd45f9584c0",
      "d6400b1c35d74040a074b59b6c821804",
      "c7d2baf532754d1e851c54a3bc98fbb1",
      "8af868322d654b139c176250efb7d446",
      "1353a7c79c3849fb8282d1b5de7d5390",
      "7aacc16491654e47a9b1671989fd7633",
      "da2300e66ff84d0884ff8d362e98bd51",
      "d50d0e30340744f289e3780b3697d9d1",
      "8cd33727bb14419c83ed27b75e82b3b5",
      "9dee23a4bae240b88f2028f1f383897a",
      "c7ba262cf709430599d57fb92770eeb5",
      "adb5dfc50ff94c01b1b5ae0b66a93014",
      "1bc9abe66b5b4dfeb40560ba687870a1",
      "73726f11287d401d95fdbbaf0da72735",
      "2661e45525d14e0db09cbf6257a2f722",
      "32397b02ae904b72850954f957fa2f01",
      "089ef5616d0848f88e7ecb4da67cd5c3",
      "7b2e8c3a7f4f4d35ba6fef48c7d12cd0",
      "6c84f0f875164b76b583c82d694cedc7",
      "a287b7d23e164a22b59f84e2b209de6b",
      "7ca53218cfe046bb9746e83e28838656",
      "673cde86ab20482e8609f0af0d36dd19",
      "d1d4c34b55eb453b92670e1585911270",
      "2ec08f2b6b82407f87f39de0a857da7a",
      "78f3ae63c0a94e0a85590d333c80af61",
      "22999ec04c2a494884607a2ae3394a5e",
      "37c745f756794bfeac41d440f36f2ed3",
      "21e89c714ea74101931fa9cb7c62c9a6",
      "1129997343c1426dabe132d38f7afad3",
      "461b1ad29f124a2a89c00a6f1dd943a2",
      "35e92f844fbb4ce9a8d2d20a31a739c5",
      "7af65c0e43834443b5a565a282e39bbb",
      "dab713411a3d4abf95232187e06d2d9b",
      "1c49424055294ff5a7da1a9bafab8d0a",
      "4b8ee73e7eee4e9b9cf38b5c64baf301",
      "abf48d9ddfea4108b94aa0e94779564f",
      "a9b2a56a2ba444f7bbdc14285e46e0a1",
      "9909d83fc0664a97a1d95f002dfdc80d",
      "c44cf18f28af4afeae117893bf125048",
      "5d1a921ecb804e63b98d7bb7895e72a7",
      "366a29755aff4ff1805281e4dfd3f699",
      "6727a48c4b29478c865cda555dcf4b24"
     ]
    },
    "id": "i--QOPRlusGJ",
    "outputId": "90e4e46a-32d3-4d86-eccd-0b5b23bf8cb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68a789c878640d09361e8071fc6ad01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecf79d06acf48c2802c6eec40604919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014a84e87f014556975c6f37bf89d7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bc48725f6c4d83887e76d4988a72e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459eda8bcf6a48a3a0832a39fdb39208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b353b56593b04fb7a6df996e512891a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ae46b528e54f538c7c3772dae77cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29375ca7b37436c9866a13ed2874fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/649k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6913f1c67bf6419c80662094a5e834f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/75.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8564511e1b2d4abbbe2d4be38b31d738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b069850b5df435e8285a3c49b3df2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb60501950dd4861950c0706e8cd2ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9cd4be0ffa402da3b376020153f887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dee23a4bae240b88f2028f1f383897a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca53218cfe046bb9746e83e28838656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af65c0e43834443b5a565a282e39bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-4143b407004a>:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkhanz23\u001b[0m (\u001b[33mkhanz23-cardiff-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250422_182411-c6a3krwq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/c6a3krwq' target=\"_blank\">./results_t5_small_mrpc_lora</a></strong> to <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/khanz23-cardiff-university/huggingface/runs/c6a3krwq' target=\"_blank\">https://wandb.ai/khanz23-cardiff-university/huggingface/runs/c6a3krwq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.053421</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.046233</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.878571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.045961</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.879433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results (LoRA): {'eval_loss': 0.045960698276758194, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.8794326241134752, 'eval_runtime': 7.6639, 'eval_samples_per_second': 53.237, 'eval_steps_per_second': 6.655, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load tokenizer and base model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "#  Apply LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],  # T5 uses 'q' and 'v' in attention modules\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# Convert model to a PEFT (LoRA-applied) model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load MRPC dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"paraphrase: {s1} </s> {s2}\"\n",
    "        for s1, s2 in zip(examples[\"sentence1\"], examples[\"sentence2\"])\n",
    "    ]\n",
    "    targets = [\n",
    "        \"equivalent\" if label == 1 else \"not_equivalent\"\n",
    "        for label in examples[\"label\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(targets, max_length=8, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize and format dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results_t5_small_mrpc_lora',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=4e-4,\n",
    "    logging_steps=50,\n",
    "    logging_dir='./logs',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=8,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    binary_preds = [1 if pred.strip() == \"equivalent\" else 0 for pred in decoded_preds]\n",
    "    binary_labels = [1 if label.strip() == \"equivalent\" else 0 for label in decoded_labels]\n",
    "\n",
    "    accuracy = accuracy_score(binary_labels, binary_preds)\n",
    "    f1 = f1_score(binary_labels, binary_preds)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.save_model('./t5_small_mrpc_lora')\n",
    "results = trainer.evaluate()\n",
    "print(\"Eval results (LoRA):\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUWVNRnh3_93"
   },
   "source": [
    "**Full-Finetuning on T5-Small:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "1I88w3qguwKy",
    "outputId": "eb4dc7a7-5cc7-434e-9dbd-dee4b84acc61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-5276f65858fa>:70: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 02:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.083625</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.842900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.034859</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.908438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.051140</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.895470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [51/51 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval results (Full Fine-Tuning): {'eval_loss': 0.03485921397805214, 'eval_accuracy': 0.875, 'eval_f1': 0.9084380610412927, 'eval_runtime': 5.7376, 'eval_samples_per_second': 71.11, 'eval_steps_per_second': 8.889, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load MRPC dataset\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f\"paraphrase: {s1} </s> {s2}\"\n",
    "        for s1, s2 in zip(examples[\"sentence1\"], examples[\"sentence2\"])\n",
    "    ]\n",
    "    targets = [\n",
    "        \"equivalent\" if label == 1 else \"not_equivalent\"\n",
    "        for label in examples[\"label\"]\n",
    "    ]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(targets, max_length=8, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize and format dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results_t5_small_mrpc_full',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=4e-4,\n",
    "    logging_steps=50,\n",
    "    logging_dir='./logs',\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=8,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    binary_preds = [1 if pred.strip() == \"equivalent\" else 0 for pred in decoded_preds]\n",
    "    binary_labels = [1 if label.strip() == \"equivalent\" else 0 for label in decoded_labels]\n",
    "\n",
    "    accuracy = accuracy_score(binary_labels, binary_preds)\n",
    "    f1 = f1_score(binary_labels, binary_preds)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "trainer.train()\n",
    "trainer.save_model('./t5_small_mrpc_full_finetuned')\n",
    "results = trainer.evaluate()\n",
    "print(\"Eval results (Full Fine-Tuning):\", results)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rWytTpDoeOxR"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
